---
title: "03.415 â€“ XGBoost Baseline by City (Multi-City, Multi-Day Forecasts with Lag Features - FIXES)"
format: html
jupyter: python3
----------------

## 0. Parameters

```{python}
import duckdb
import pandas as pd
import numpy as np
from pathlib import Path
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit # Import necessary tools

# --- 0. Parameters (unchanged) ---
city        = "Chattanooga"
train_start = "2015-01-01"
train_end   = "2022-12-31"
test_start  = "2023-01-01"
test_end    = "2023-12-31"
project_root = Path().resolve().parent
db_path      = project_root / "weather.duckdb"
assert db_path.exists(), f"DuckDB not found at {db_path}"
print(f"DB path:       {db_path}")
print(f"Train window:  {train_start} â†’ {train_end}")
print(f"Test window:   {test_start} â†’ {test_end}")

# --- 1. Load & Clean Data (unchanged) ---
query = f"""
SELECT
  date, location, lat, lon, t2m, t2m_max, t2m_min, t2m_range, dewpoint, rh2m,
  ws10m, wd10m, ps, allsky_sw_dwn, allsky_lw_dwn, prectotcorr
FROM weather
WHERE date BETWEEN '{train_start}' AND '{test_end}'
ORDER BY date, location
"""
df = duckdb.connect(str(db_path)).execute(query).fetchdf()

numeric_cols = [
    "t2m","t2m_max","t2m_min","t2m_range","dewpoint","rh2m","ws10m",
    "wd10m","ps","allsky_sw_dwn","allsky_lw_dwn","prectotcorr",
    "lat", "lon"
]
df["date"] = pd.to_datetime(df["date"])
df = df[(df[numeric_cols]!= -999).all(axis=1)]
print(f"After initial cleaning: {df.shape[0]} rows")
print(f"Number of unique locations: {df['location'].nunique()}")

# --- 2. Feature Engineering (unchanged logic, for a complete example) ---
df["date_ordinal"] = df["date"].map(pd.Timestamp.toordinal)
df["day_of_year"] = df["date"].dt.dayofyear
df["doy_sin"] = np.sin(2 * np.pi * df["day_of_year"] / 365)
df["doy_cos"] = np.cos(2 * np.pi * df["day_of_year"] / 365)

dynamic_weather_features = [
    "t2m", "t2m_min", "t2m_max", "t2m_range", "dewpoint", "rh2m",
    "ws10m", "wd10m", "ps", "allsky_sw_dwn", "allsky_lw_dwn", "prectotcorr"
]
lags = [1, 2, 3, 4, 5, 6, 7]

for feature in dynamic_weather_features:
    for lag in lags:
        new_col_name = f"{feature}_lag_{lag}"
        df[new_col_name] = df.groupby('location')[feature].shift(lag)

df['t2m_max_1_day_later'] = df.groupby('location')['t2m_max'].shift(-1)
df['t2m_max_3_days_later'] = df.groupby('location')['t2m_max'].shift(-3)
df['t2m_max_5_days_later'] = df.groupby('location')['t2m_max'].shift(-5)

df = pd.get_dummies(df, columns=['location'], prefix='location', dtype=int)

feature_cols = [
    "date_ordinal", "day_of_year", "doy_sin", "doy_cos", "lat", "lon",
    "t2m", "t2m_min", "t2m_max", "t2m_range", "dewpoint", "rh2m",
    "ws10m", "wd10m", "ps", "allsky_sw_dwn", "allsky_lw_dwn", "prectotcorr"
]
for feature in dynamic_weather_features:
    for lag in lags:
        feature_cols.append(f"{feature}_lag_{lag}")
location_ohe_cols = [col for col in df.columns if col.startswith('location_')]
feature_cols.extend(location_ohe_cols)

target_cols = [
    "t2m_max_1_day_later", "t2m_max_3_days_later", "t2m_max_5_days_later"
]

all_cols_to_check_for_nan = target_cols + [f"{feat}_lag_{lag}" for feat in dynamic_weather_features for lag in lags]
original_rows = df.shape[0]
df.dropna(subset=all_cols_to_check_for_nan, inplace=True)
print(f"Dropped {original_rows - df.shape[0]} rows due to NaNs from shifting targets and lags.")
print("Features:", feature_cols)
print("Targets: ", target_cols)

# --- 3. Train/Test Split (unchanged) ---
mask_train = (df["date"] >= train_start) & (df["date"] <= train_end)
mask_test  = (df["date"] >= test_start)  & (df["date"] <= test_end)
train_df = df[mask_train].reset_index(drop=True)
test_df  = df[mask_test].reset_index(drop=True)
print("Train rows:", train_df.shape[0])
print("Test rows: ", test_df.shape[0])

# --- NEW SECTION: Hyperparameter Tuning ---
print("\n--- Starting Hyperparameter Tuning ---")

# Define the model to tune (e.g., 1-day forecast model)
target_to_tune = "t2m_max_1_day_later"
iX_train = train_df[feature_cols]
iy_train = train_df[target_to_tune]

# Define the parameter grid to search
# Start with a relatively small grid, then expand or refine
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Initialize the XGBoost Regressor
# Set random_state for reproducibility
xgb_model = XGBRegressor(random_state=0)

# Set up TimeSeriesSplit for cross-validation
# n_splits: number of train/test splits.
# The `max_train_size` can be useful for very long time series to keep training window consistent.
# For simplicity, let's use n_splits.
tscv = TimeSeriesSplit(n_splits=5) # Example: 5 splits, each split uses increasing past data

# Set up GridSearchCV
# scoring='neg_root_mean_squared_error' is common for regression (GridSearchCV minimizes by default)
# So, it looks for the lowest RMSE.
grid_search = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    scoring='neg_root_mean_squared_error', # Use negative RMSE for optimization
    cv=tscv,
    n_jobs=-1, # Use all available CPU cores
    verbose=2 # Show progress
)

# Perform the grid search
print(f"Running Grid Search for {target_to_tune}...")
grid_search.fit(iX_train, iy_train)

print("\n--- Tuning Complete ---")
print(f"Best parameters for {target_to_tune}: {grid_search.best_params_}")
print(f"Best RMSE (negative value, so lower is better): {-grid_search.best_score_:.3f}") # Convert back to positive RMSE

# You can now use grid_search.best_estimator_ as your trained model for this target
# For subsequent sections, you would replace the direct XGBRegressor instantiation
# with the best_estimator_ from the grid search.

# Example: Using the best estimator for the 1-day forecast model
best_model_1day = grid_search.best_estimator_

# If you want to tune all three models, you'd repeat the GridSearchCV process
# for each target_col, storing their best_estimator_ in the 'models' dictionary.
# For simplicity in this example, we only tuned one.

# --- 4. Train XGBoost Models (Modified to use best_estimator if tuned) ---
print("\n--- Re-training/Using Tuned Models ---")
models = {}
# For the tuned model, use the best_estimator_
models[target_to_tune] = best_model_1day
print(f"Model for {target_to_tune} is now the best-tuned estimator.")

# For other models, if not tuned, you'd instantiate them as before
for target_col in target_cols:
    if target_col not in models: # If not already tuned and added
        print(f"\nTraining default model for: {target_col}")
        iX_train_other = train_df[feature_cols]
        iy_train_other = train_df[target_col]
        model = XGBRegressor(
            n_estimators=100, # Default if not tuned
            learning_rate=0.1, # Default if not tuned
            random_state=0
        )
        model.fit(iX_train_other, iy_train_other)
        models[target_col] = model
        print(f"Model for {target_col} training complete.")

print("\nAll models ready.")


# --- 5. Forecast & Evaluate (unchanged) ---
df_eval = pd.DataFrame({"date": test_df["date"]})

for target_col in target_cols:
    X_test = test_df[feature_cols]
    y_true = test_df[target_col]
    y_pred = models[target_col].predict(X_test)

    df_eval[f"y_true_{target_col}"] = y_true
    df_eval[f"y_pred_{target_col}"] = y_pred

    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae  = mean_absolute_error(y_true, y_pred)
    print(f"\nMetrics for {target_col}:")
    print(f"  RMSE: {rmse:.3f}, MAE: {mae:.3f}")

# --- 6. Plot Results (unchanged) ---
# ... (plotting code as in previous response)
import plotly.express as px

base_blue_1 = "#007BFF" # Darker blue for 1-day
base_blue_3 = "#66B2FF" # Medium blue for 3-day
base_blue_5 = "#ADD8FF" # Lighter blue for 5-day

chattanooga_mask_full_df = df['location_Chattanooga'] == 1
df_chattanooga_actual = df[chattanooga_mask_full_df].copy()

chattanooga_mask_test_df = test_df['location_Chattanooga'] == 1
test_df_chattanooga = test_df[chattanooga_mask_test_df].reset_index(drop=True)

temp_df_eval_chattanooga = pd.DataFrame({"date": test_df_chattanooga["date"]})

for target_col in target_cols:
    X_test_chattanooga = test_df_chattanooga[feature_cols]
    y_true_chattanooga = test_df_chattanooga[target_col]
    y_pred_chattanooga = models[target_col].predict(X_test_chattanooga)

    temp_df_eval_chattanooga[f"y_true_{target_col}"] = y_true_chattanooga
    temp_df_eval_chattanooga[f"y_pred_{target_col}"] = y_pred_chattanooga


# --- First Plot (Original) ---
plot_df_list_original = []
plot_df_list_original.append(
    df_chattanooga_actual[["date", "t2m_max"]].assign(series="Actual (Today's Max)", type="Actual_Today")
    .rename(columns={"t2m_max": "temperature"})
)
for target_col in target_cols:
    horizon_name = target_col.replace('t2m_max_', '').replace('_later', '')
    if horizon_name == "1 day":
        plot_df_list_original.append(
            temp_df_eval_chattanooga[["date", f"y_true_{target_col}"]].assign(
                series=f"Test Actual ({horizon_name})", type=f"Test_Actual_{horizon_name}"
            ).rename(columns={f"y_true_{target_col}": "temperature"})
        )
    plot_df_list_original.append(
        temp_df_eval_chattanooga[["date", f"y_pred_{target_col}"]].assign(
            series=f"Test Pred ({horizon_name})", type=f"Test_Pred_{horizon_name}"
        ).rename(columns={f"y_pred_{target_col}": "temperature"})
    )
plot_df_original = pd.concat(plot_df_list_original, ignore_index=True)
color_map_original = {
    "Actual (Today's Max)": "black", "Test Actual (1 day)": base_blue_1,
    "Test Pred (1 day)": "#3399FF", "Test Pred (3 days)": "#99CCFF", "Test Pred (5 days)": "#CCEEFF",
}
line_dash_map_original = {
    "Actual (Today's Max)": "solid", "Test Actual (1 day)": "solid",
    "Test Pred (1 day)": "dot", "Test Pred (3 days)": "dot", "Test Pred (5 days)": "dot",
}
fig_original = px.line(
    plot_df_original, x="date", y="temperature", color="series",
    title=f"XGBoost Baseline for {city} (Trained on Multiple Cities)", template="plotly_white",
    color_discrete_map=color_map_original, line_dash_map=line_dash_map_original
)
fig_original.update_layout(xaxis_title="Date", yaxis_title="T2M max (Â°C)")
fig_original.show()

# --- Second Plot (Shifted 1-Day Prediction) ---
plot_df_shifted = []
plot_df_shifted.append(
    df_chattanooga_actual[["date", "t2m_max"]].assign(series="Actual (Today's Max)")
    .rename(columns={"t2m_max": "temperature"})
)
plot_df_shifted.append(
    temp_df_eval_chattanooga[["date", "y_true_t2m_max_1_day_later"]].assign(series="Test Actual (1 day)")
    .rename(columns={"y_true_t2m_max_1_day_later": "temperature"})
)
test_pred_1day_shifted = temp_df_eval_chattanooga[["date", "y_pred_t2m_max_1_day_later"]].copy()
test_pred_1day_shifted['date'] = test_pred_1day_shifted['date'] + pd.Timedelta(days=1)
test_pred_1day_shifted['series'] = "Test Pred (1 day) - Shifted"
test_pred_1day_shifted = test_pred_1day_shifted.rename(columns={"y_pred_t2m_max_1_day_later": "temperature"})
plot_df_shifted.append(test_pred_1day_shifted)
plot_df_shifted = pd.concat(plot_df_shifted, ignore_index=True)
color_map_shifted = {
    "Actual (Today's Max)": "black", "Test Actual (1 day)": base_blue_1, "Test Pred (1 day) - Shifted": "red",
}
line_dash_map_shifted = {
    "Actual (Today's Max)": "solid", "Test Actual (1 day)": "solid", "Test Pred (1 day) - Shifted": "dash",
}
fig_shifted = px.line(
    plot_df_shifted, x="date", y="temperature", color="series",
    title=f"XGBoost 1-Day Forecast Accuracy for {city} (Trained on Multiple Cities) - Shifted Prediction",
    template="plotly_white", color_discrete_map=color_map_shifted, line_dash_map=line_dash_map_shifted
)
fig_shifted.update_layout(xaxis_title="Date", yaxis_title="T2M max (Â°C)")
fig_shifted.show()
```


